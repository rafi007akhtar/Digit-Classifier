{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Classifier\n",
    "\n",
    "This Notebook uses a neural network that inputs an image of a handwritten digit, and predicts what digit it is (0 to 10).\n",
    "\n",
    "I am making this classifier while learning the basics of DL and neural networks online, from Micheal Nielsen's online book, the link of which is available in the README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\" Return sigma(z), where z = w.x + b \"\"\"\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\" Return sigma'(z), where z = w.x + b \"\"\"\n",
    "    sigma = sigmoid(z)\n",
    "    return sigma * (1-sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Network` class\n",
    "\n",
    "The following class represents a basic neural network, with attributes and methods as follows.\n",
    "\n",
    "### Attributes\n",
    "<ul>\n",
    "    <li> sizes </li>\n",
    "    <li> number of layers </li>\n",
    "    <li> biases </li>\n",
    "    <li> weights </li>\n",
    "</ul>\n",
    "\n",
    "### Methods\n",
    "\n",
    "<ul>\n",
    "    <li> `feedforward`: compute Ïƒ(wx + b) for the neural network </li>\n",
    "    <li> `update_mini_batch`: perform gradient descent and backpropagation on a mini-batch </li>\n",
    "    <li> `backpropagate`: compute gradients of cost on weights and biases </li>\n",
    "    <li> `SGD`: perform stochastic gradient descent </li>\n",
    "    <li> `evaulate`: test the network on test-set </li>\n",
    "</ul>\n",
    "\n",
    "### Working\n",
    "\n",
    "You can create a neural network by creating an instance of the following class:\n",
    "\n",
    "```\n",
    "    myNetwork = Network(size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Constructor to initalize the object attributes\n",
    "        Assumption: layer 0 is the input layer (so it won't have any biases)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.size = size # number of neurons in each layer\n",
    "        self.num_layers = len(size) # number of layers\n",
    "        self.biases = [np.random.randn(l, 1) for l in size[1:]] # randomly initalize bias of each layer from layer 1\n",
    "        self.weights = [np.random.randn(l, m) for m, l in zip(size[:-1], size[1:])] # randomly initiliaze weights in the same way\n",
    "        \n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        \"\"\" \n",
    "        Compute sigma(w.x + b), where w is list of weights for each layer, and b is the list of biases for each layer.\n",
    "        Return the feedforward output after going through all layers\n",
    "        \"\"\"\n",
    "        \n",
    "        for w,b in zip(self.weights, self.biases):\n",
    "            a = sigmoid(np.dot(w,a) + b)\n",
    "        return a\n",
    "    \n",
    "    def backdrop(self, x, y):\n",
    "        \"\"\"\n",
    "        Return the gradients nabla_b, nabla_w for the baises and weights respectivly, applying backpropagation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the tuples with zeroes\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]        \n",
    "        \n",
    "        # Feedforward\n",
    "        act = x # activation for first layer\n",
    "        acts = [x] # list to store activations for all layers\n",
    "        Z = [] # list to store all z's layer-wise\n",
    "        for b,w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, act) + b\n",
    "            Z.append(z)\n",
    "            act = sigmoid(z)\n",
    "            acts.append(act)\n",
    "        \n",
    "        # Compute the error\n",
    "        delta = self.cost_prime(acts[-1], y) * sigmoid_prime(Z[-1])\n",
    "        \n",
    "        # Compute the final gradients\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, acts[-2].transpose())\n",
    "        \n",
    "        # Backpropagation\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = Z[-l]\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sigmoid_prime(z)\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, acts[-l-1].transpose())\n",
    "        \n",
    "        # results\n",
    "        return nabla_b, nabla_w           \n",
    "        \n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Calculate the gradients (or derivatives) for all weights and biases in the mini-batch using backpropagation.\n",
    "        Apply gradient descent to update the entries of the mini-batch using the gradients obtained, thereby updating the weights and biases of the network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mini_batch: a list of tuples (x,y) randomly selected from the training set\n",
    "        eta: the learning rate of the algorithm\n",
    "        \"\"\"\n",
    "        \n",
    "        # gradient arrays for weights and biases, initialized with zeroes\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights] \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        for x,y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backdrop(x,y) # compute derivatives\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        \n",
    "        # update weights and biases\n",
    "        m = len(mini_batch)\n",
    "        self.weights = [w - (eta*nw)/m for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta*nb)/m for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    \n",
    "    def SGD(self, train_data, epochs, mini_batch_size, eta, test_data = None):\n",
    "        \"\"\" Stochastic Gradient Descent\n",
    "        Aim: train the neural network using stochastic gradient descent in mini-batches\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        train_data: list of (x,y) tuples listing the inputs (x) and their desired outputs (y);\n",
    "        epochs: the point where training of one mini-batch ends, and another needs to begin;\n",
    "        mini_batch_size: number of elements in the mini-batch;\n",
    "        eta: learning rate;\n",
    "        test_data (optional): if provided, the network will be evaluated with the test data after each epoch, and progress will be printed\n",
    "        \"\"\"\n",
    "        \n",
    "        train_data = list(train_data)\n",
    "        n = len(train_data)\n",
    "        \n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            m = len(test_data)\n",
    "        \n",
    "        # loop over every epoch until the training set is exhausted\n",
    "        for epoch in range(epochs):\n",
    "            # pick random samples from the training set, and put them in mini-batches\n",
    "            random.shuffle(train_data)\n",
    "            mini_batches = [train_data[i: i+mini_batch_size] for i in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            # perform gradient descent and back-propagation in every mini-batch\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "            # test the trained batches, if test set is provided\n",
    "            if test_data:\n",
    "                print (f'Epoch {epoch}: {self.evaluate(test_data)} / {m}')\n",
    "            else:\n",
    "                print (f'Epoch {epoch} complete')\n",
    "    \n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\" \n",
    "        Test our trained data against the training batch provided.\n",
    "        Return the number of test cases that result in correct output from the neural network.\n",
    "        Neural networks output is the index of the neuron in the final layer having the highest activation.\n",
    "        \"\"\"\n",
    "        \n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for x, y in test_data]\n",
    "        return sum(x==y for x,y in test_results)\n",
    "    \n",
    "    \n",
    "    def cost_prime(self, a, y):\n",
    "        \"\"\" Return cost derivative C'(x) = (a-y) for n = 1 \"\"\"\n",
    "        return a-y\n",
    "        \n",
    "    \n",
    "    def printElements(self):\n",
    "        \"\"\" Print the elements of the current instance of the neural network \"\"\"\n",
    "        \n",
    "        print(f'This neural network has: \\n{self.num_layers} number of layers, \\n{self.size} neurons in each layer, \\n{self.biases} as biases for each layer, and \\n{self.weights} as weights for each layer.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following lines to understand how a zip works\n",
    "# l1 = [1,2,3]\n",
    "# l2 = [4,5,6]\n",
    "# print(f'first zip: {list(zip(l1,l2))}')\n",
    "# for i in zip(l1,l2):\n",
    "#     print(i)\n",
    "\n",
    "# l = [1,2,3,4,5]\n",
    "# print(f'second zip: {list(zip(l, l[1:]))}')\n",
    "# print(np.multiply(l1,l2))\n",
    "# print(list(zip(l1,l2)))\n",
    "\n",
    "# np.random.randn(1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the classifier to recognize handwritten digits\n",
    "\n",
    "The class `Network` contains all the members needed to run the classifier.\n",
    "\n",
    "** Steps. **\n",
    "<ol>\n",
    "    <li> At first, import the helper file `mnist_loader` file to the notebook </li>\n",
    "    <li> Now, load the train, validation and test data from the the helper </li>\n",
    "    <li> Set up a network with `30` neurons </li>\n",
    "    <li> Perform Stochastic Gradient Descent on it with initial parameters for 30 epochs </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9111 / 10000\n",
      "Epoch 1: 9238 / 10000\n",
      "Epoch 2: 9296 / 10000\n",
      "Epoch 3: 9339 / 10000\n",
      "Epoch 4: 9396 / 10000\n",
      "Epoch 5: 9372 / 10000\n",
      "Epoch 6: 9419 / 10000\n",
      "Epoch 7: 9434 / 10000\n",
      "Epoch 8: 9440 / 10000\n",
      "Epoch 9: 9441 / 10000\n",
      "Epoch 10: 9449 / 10000\n",
      "Epoch 11: 9469 / 10000\n",
      "Epoch 12: 9477 / 10000\n",
      "Epoch 13: 9485 / 10000\n",
      "Epoch 14: 9423 / 10000\n",
      "Epoch 15: 9494 / 10000\n",
      "Epoch 16: 9477 / 10000\n",
      "Epoch 17: 9463 / 10000\n",
      "Epoch 18: 9509 / 10000\n",
      "Epoch 19: 9515 / 10000\n",
      "Epoch 20: 9480 / 10000\n",
      "Epoch 21: 9504 / 10000\n",
      "Epoch 22: 9511 / 10000\n",
      "Epoch 23: 9510 / 10000\n",
      "Epoch 24: 9503 / 10000\n",
      "Epoch 25: 9490 / 10000\n",
      "Epoch 26: 9493 / 10000\n",
      "Epoch 27: 9487 / 10000\n",
      "Epoch 28: 9509 / 10000\n",
      "Epoch 29: 9510 / 10000\n"
     ]
    }
   ],
   "source": [
    "# Load the train, test and validation data from mnist_loader file\n",
    "train_set, validation_set, test_set = mnist_loader.load_data_wrapper()\n",
    "\n",
    "# Set up a network with 30 neurons in the hidden layer\n",
    "myNet = Network([784, 30, 10])\n",
    "\n",
    "# Perform stochastic gradient descent\n",
    "myNet.SGD(train_set, epochs = 30, mini_batch_size = 10, eta = 3.0, test_data = test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This brings this notebook to an end. By the end of the learning, the neural network managed to attain an accuracy of **95.10 %**, which is satisfactory for starters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
