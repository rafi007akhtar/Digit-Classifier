{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Classifier\n",
    "\n",
    "This Notebook uses a neural network that inputs an image of a handwritten digit, and predicts what digit it is (0 to 10).\n",
    "\n",
    "I am making this classifier while learning the basics of DL and neural networks online, from Micheal Nielsen's online book, the link of which is available in the README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"return sigma(z)\"\"\"\n",
    "    return 1.0/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Network` class\n",
    "\n",
    "The following class represents a basic neural network, with attributes and methods as follows.\n",
    "\n",
    "### Attributes\n",
    "<ul>\n",
    "    <li> sizes </li>\n",
    "    <li> number of layers </li>\n",
    "    <li> biases </li>\n",
    "    <li> weights </li>\n",
    "</ul>\n",
    "\n",
    "### Methods\n",
    "\n",
    "<ul>\n",
    "    <li> `feedforward`: compute Ïƒ(wx + b) for the neural network </li>\n",
    "    <li> `update_mini_batch`: perform gradient descent and backpropagation on a mini-batch </li>\n",
    "    <li> `SGD`: perform stochastic gradient descent </li>\n",
    "    <li> `evaulate`: test the network on test-set </li>\n",
    "</ul>\n",
    "\n",
    "### Working\n",
    "\n",
    "You can create a neural network by creating an instance of the following class:\n",
    "\n",
    "```\n",
    "    myNetwork = Network(size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Constructor to initalize the object attributes\n",
    "        Assumption: layer 0 is the input layer (so it won't have any biases)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.size = size # number of neurons in each layer\n",
    "        self.num_layers = len(size) # number of layers\n",
    "        self.biases = [np.random.randn(1, l) for l in size[1:]] # randomly initalize bias of each layer from layer 1\n",
    "        self.weights = [np.random.randn(l, m) for m, l in zip(size[:-1], size[1:])] # randomly initiliaze weights in the same way\n",
    "        \n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        \"\"\" \n",
    "        Compute sigma(w.x + b), where w is list of weights for each layer, and b is the list of biases for each layer.\n",
    "        Return the feedforward output after going through all layers\n",
    "        \"\"\"\n",
    "        for w,b in zip(self.weights, self.biases):\n",
    "            y = sigmoid(np.dot(w,x) + b)\n",
    "        return y\n",
    "    \n",
    "    def backpropagate(self, x, y):\n",
    "        # TODO: Write the code for backpropagation\n",
    "        pass\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Calculate the gradients (or derivatives) for all weights and biases in the mini-batch using backpropagation.\n",
    "        Apply gradient descent to update the entries of the mini-batch using the gradients obtained, thereby updating the weights and biases of the network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mini_batch: a list of tuples (x,y) randomly selected from the training set\n",
    "        eta: the learning rate of the algorithm\n",
    "        \"\"\"\n",
    "        \n",
    "        # gradient arrays for weights and biases, initialized with zeroes\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights] \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        for x,y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backdrop(x,y) # compute derivatives\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        \n",
    "        # update weights and biases\n",
    "        m = len(mini_batch)\n",
    "        self.weights = [w - (eta*nw)/m for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta*nb)/m for b, m in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    \n",
    "    def SGD(self, train_data, epochs, mini_batch_size, eta, test_data = None):\n",
    "        \"\"\" Stochastic Gradient Descent\n",
    "        Aim: train the neural network using stochastic gradient descent in mini-batches\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        train_data: list of (x,y) tuples listing the inputs (x) and their desired outputs (y);\n",
    "        epochs: the point where training of one mini-batch ends, and another needs to begin;\n",
    "        mini_batch_size: number of elements in the mini-batch;\n",
    "        eta: learning rate;\n",
    "        test_data (optional): if provided, the network will be evaluated with the test data after each epoch, and progress will be printed\n",
    "        \"\"\"\n",
    "        \n",
    "        n = len(train_data)\n",
    "        \n",
    "        # loop over every epoch until the training set is exhausted\n",
    "        for epoch in range(epochs):\n",
    "            # pick random samples from the training set, and put them in mini-batches\n",
    "            random.shuffle(train_data)\n",
    "            mini_batches = [train_data[i: i+mini_batch_size] for i in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            # perform gradient descent and back-propagation in every mini-batch\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "            # test the trained batches, if test set is provided\n",
    "            if test_data:\n",
    "                m = len(test_data)\n",
    "                print (f'Epoch {epoch}: {self.evaluate(test_data)} / {m}')\n",
    "            else:\n",
    "                print (f'Epoch {epoch} complete')\n",
    "    \n",
    "    \n",
    "    def evaluate(test_data):\n",
    "        pass\n",
    "                \n",
    "        \n",
    "    \n",
    "    def printElements(self):\n",
    "        \"\"\" Print the elements of the current instance of the neural network \"\"\"\n",
    "        \n",
    "        print(f'This neural network has: \\n{self.num_layers} number of layers, \\n{self.size} neurons in each layer, \\n{self.biases} as biases for each layer, and \\n{self.weights} as weights for each layer.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 10 18]\n",
      "[(1, 4), (2, 5), (3, 6)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.21487966, -1.57789714,  0.01869331]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment the following lines to understand how a zip works\n",
    "l1 = [1,2,3]\n",
    "l2 = [4,5,6]\n",
    "# print(f'first zip: {list(zip(l1,l2))}')\n",
    "# for i in zip(l1,l2):\n",
    "#     print(i)\n",
    "\n",
    "# l = [1,2,3,4,5]\n",
    "# print(f'second zip: {list(zip(l, l[1:]))}')\n",
    "print(np.multiply(l1,l2))\n",
    "print(list(zip(l1,l2)))\n",
    "\n",
    "np.random.randn(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This neural network has: \n",
      "3 number of layers, \n",
      "[2, 3, 1] neurons in each layer, \n",
      "[array([[-1.35151069, -0.74583158,  1.3762929 ]]), array([[ 0.58943436]])] as biases for each layer, and \n",
      "[array([[ 0.0280287 ,  1.39417031],\n",
      "       [ 0.86183564,  1.16321671],\n",
      "       [ 0.49095898,  0.45971998]]), array([[-0.23935568, -0.26587206,  2.9051824 ]])] as weights for each layer.\n"
     ]
    }
   ],
   "source": [
    "myNet = Network([2,3,1])\n",
    "myNet.printElements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = [np.zeros(b.shape) for b in myNet.biases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  0.,  0.]]), array([[ 0.]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
